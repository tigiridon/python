{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fabulous-issue",
   "metadata": {},
   "source": [
    "#### From SAV to love"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-christianity",
   "metadata": {},
   "source": [
    "# HW №1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-circular",
   "metadata": {},
   "source": [
    "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания. Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию:\n",
    "\n",
    "для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    "для для замены @user на пробел, необходимо использовать re.sub() при применении функции необходимо использовать np.vectorize(function).\n",
    "2) Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "3) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "4) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "5) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "7) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "8) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "9) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "\n",
    "10) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "11) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "12) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "13) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "14) Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "import pickle\n",
    "import pymystem3\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prescribed-barrier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "respected-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_tweets.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-stock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...\n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3    0.0                                bihday your majesty\n",
       "3   4    0.0  #model   i love u take with u all the time in ...\n",
       "4   5    0.0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "auburn-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MorphAnalyzer()\n",
    "\n",
    "# убираем все небуквенные символы\n",
    "regex = re.compile(\"[А-Яа-яA-z]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return regex.findall(text.lower())\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "democratic-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(token, pymorphy=m):\n",
    "    return pymorphy.parse(token)[0].normal_form\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_word(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bright-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = words_only(combine_df.tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "judicial-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = stopwords.words('english')\n",
    "def remove_stopwords(lemmas, stop_words = mystopwords):\n",
    "    return [w for w in lemmas if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "soviet-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = lemmatize_text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "about-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lemmas, stopwords = mystopwords):\n",
    "    return [w for w in lemmas if not w in stopwords and len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recent-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    tokens = words_only(text)\n",
    "    lemmas = lemmatize_text(tokens)\n",
    "    \n",
    "    return remove_stopwords(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "medium-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_express = re.compile('@[\\w]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intelligent-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet']=combine_df['tweet'].str.replace(re_express, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "exceptional-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet']=combine_df['tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "imperial-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_express1 = re.compile('[^\\w\\s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "massive-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet']=combine_df['tweet'].str.replace(re_express1, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "greenhouse-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_express2 = re.compile('[^a-zA-Z0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "analyzed-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet']=combine_df['tweet'].str.replace(re_express2, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "arbitrary-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_express3 = re.compile('[^a-zA-Z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aerial-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet']=combine_df['tweet'].str.replace(re_express3, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "complimentary-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet']= combine_df['tweet'].str.findall('\\w{1,}').str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caroline-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "combine_df['tweet_token'] = combine_df['tweet'].apply(tt.tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "raised-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "copyrighted-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet_token_filtered'] = combine_df['tweet_token'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hydraulic-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "middle-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet_stemmed'] = combine_df['tweet_token_filtered'].apply(lambda x: [stemmer.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cooperative-manchester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when a father is dysfunctional and is so selfi...</td>\n",
       "      <td>[when, a, father, is, dysfunctional, and, is, ...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit i can t use cause they ...</td>\n",
       "      <td>[thanks, for, lyft, credit, i, can, t, use, ca...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>model i love u take with u all the time in ur</td>\n",
       "      <td>[model, i, love, u, take, with, u, all, the, t...</td>\n",
       "      <td>[model, love, u, take, u, time, ur]</td>\n",
       "      <td>[model, love, u, take, u, time, ur]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>[factsguide, society, now, motivation]</td>\n",
       "      <td>[factsguide, society, motivation]</td>\n",
       "      <td>[factsguid, societi, motiv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>huge fan fare and big talking before they leav...</td>\n",
       "      <td>[huge, fan, fare, and, big, talking, before, t...</td>\n",
       "      <td>[huge, fan, fare, big, talking, leave, chaos, ...</td>\n",
       "      <td>[huge, fan, fare, big, talk, leav, chao, pay, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>camping tomorrow danny</td>\n",
       "      <td>[camping, tomorrow, danny]</td>\n",
       "      <td>[camping, tomorrow, danny]</td>\n",
       "      <td>[camp, tomorrow, danni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the next school year is the year for exams can...</td>\n",
       "      <td>[the, next, school, year, is, the, year, for, ...</td>\n",
       "      <td>[next, school, year, year, exams, think, schoo...</td>\n",
       "      <td>[next, school, year, year, exam, think, school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>we won love the land allin cavs champions clev...</td>\n",
       "      <td>[we, won, love, the, land, allin, cavs, champi...</td>\n",
       "      <td>[love, land, allin, cavs, champions, cleveland...</td>\n",
       "      <td>[love, land, allin, cav, champion, cleveland, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>welcome here i m it s so gr</td>\n",
       "      <td>[welcome, here, i, m, it, s, so, gr]</td>\n",
       "      <td>[welcome, gr]</td>\n",
       "      <td>[welcom, gr]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when a father is dysfunctional and is so selfi...   \n",
       "1   2    0.0  thanks for lyft credit i can t use cause they ...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0      model i love u take with u all the time in ur   \n",
       "4   5    0.0                  factsguide society now motivation   \n",
       "5   6    0.0  huge fan fare and big talking before they leav...   \n",
       "6   7    0.0                             camping tomorrow danny   \n",
       "7   8    0.0  the next school year is the year for exams can...   \n",
       "8   9    0.0  we won love the land allin cavs champions clev...   \n",
       "9  10    0.0                        welcome here i m it s so gr   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, a, father, is, dysfunctional, and, is, ...   \n",
       "1  [thanks, for, lyft, credit, i, can, t, use, ca...   \n",
       "2                            [bihday, your, majesty]   \n",
       "3  [model, i, love, u, take, with, u, all, the, t...   \n",
       "4             [factsguide, society, now, motivation]   \n",
       "5  [huge, fan, fare, and, big, talking, before, t...   \n",
       "6                         [camping, tomorrow, danny]   \n",
       "7  [the, next, school, year, is, the, year, for, ...   \n",
       "8  [we, won, love, the, land, allin, cavs, champi...   \n",
       "9               [welcome, here, i, m, it, s, so, gr]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "3                [model, love, u, take, u, time, ur]   \n",
       "4                  [factsguide, society, motivation]   \n",
       "5  [huge, fan, fare, big, talking, leave, chaos, ...   \n",
       "6                         [camping, tomorrow, danny]   \n",
       "7  [next, school, year, year, exams, think, schoo...   \n",
       "8  [love, land, allin, cavs, champions, cleveland...   \n",
       "9                                      [welcome, gr]   \n",
       "\n",
       "                                       tweet_stemmed  \n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...  \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...  \n",
       "2                                  [bihday, majesti]  \n",
       "3                [model, love, u, take, u, time, ur]  \n",
       "4                        [factsguid, societi, motiv]  \n",
       "5  [huge, fan, fare, big, talk, leav, chao, pay, ...  \n",
       "6                            [camp, tomorrow, danni]  \n",
       "7  [next, school, year, year, exam, think, school...  \n",
       "8  [love, land, allin, cav, champion, cleveland, ...  \n",
       "9                                       [welcom, gr]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ceramic-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df['tweet_lemmatized'] = lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "innovative-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df.to_pickle(\"./combine_df7.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
